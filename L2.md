 
### **WzÃ³r metody Jacobiego**
Metoda Jacobiego to iteracyjny sposÃ³b rozwiÄ…zywania ukÅ‚adu rÃ³wnaÅ„ liniowych:

$$
A \mathbf{x} = \mathbf{b}
$$

gdzie:
- **$A$** â€“ macierz kwadratowa $n \times n$.
- **$\mathbf{x}$** â€“ wektor niewiadomych $x_1, x_2, ..., x_n$.
- **$\mathbf{b}$** â€“ wektor wyrazÃ³w wolnych.

WzÃ³r iteracyjny metody Jacobiego:

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{\substack{j=1 \\ j\neq i}}^{n} a_{ij} x_j^{(k)} \right), \quad i = 1,2, \dots, n
$$

### **WyjaÅ›nienie oznaczeÅ„:**
1. **$x_i^{(k+1)}$** â€“ nowe przybliÅ¼enie wartoÅ›ci niewiadomej $x_i$ w iteracji $k+1$.


2. **$a_{ii}$** â€“ element diagonalny macierzy $A$, czyli $i$-ta wartoÅ›Ä‡ na gÅ‚Ã³wnej przekÄ…tnej.  
3. **$b_i$** â€“ $i$-ta wartoÅ›Ä‡ wektora $\mathbf{b}$ (wyrazy wolne).  
4. **$a_{ij}$** â€“ element macierzy $A$ (dla $j \neq i$, czyli poza diagonalÄ…).  
5. **$x_j^{(k)}$** â€“ wartoÅ›Ä‡ niewiadomej $x_j$ z **poprzedniej iteracji** $k$.  
6. **$\sum_{\substack{j=1 \\ j \neq i}}^{n} a_{ij} x_j^{(k)}$** â€“ suma wartoÅ›ci $a_{ij} x_j^{(k)}$ dla wszystkich zmiennych oprÃ³cz $x_i$.  
7. **$i$** â€“ indeks numeru rÃ³wnania i zmiennej, ktÃ³rÄ… obliczamy w danym kroku iteracyjnym  
   - W ukÅ‚adzie rÃ³wnaÅ„ mamy $n$ rÃ³wnaÅ„ i $n$ zmiennych.  
   - $i$ przyjmuje wartoÅ›ci od 1 do $n$, oznaczajÄ…c kolejne zmienne $x_1, x_2, ..., x_n$.  
   - Dla kaÅ¼dego $i$ obliczamy nowÄ… wartoÅ›Ä‡ $x_i^{(k+1)}$, uÅ¼ywajÄ…c poprzednich wartoÅ›ci $x_j^{(k)}$ (gdzie $j \neq i$).  
8. **$n$** â€“ liczba rÃ³wnaÅ„ i zmiennych w ukÅ‚adzie  
   - Macierz $A$ ma wymiar $n \times n$, co oznacza, Å¼e mamy dokÅ‚adnie $n$ rÃ³wnaÅ„ i $n$ niewiadomych.  
   - Wektor $\mathbf{x}$ zawiera $n$ zmiennych, ktÃ³re sÄ… iteracyjnie obliczane.  
   - Proces iteracyjny wykonuje siÄ™ dla wszystkich $i = 1, 2, ..., n$, aÅ¼ do osiÄ…gniÄ™cia odpowiedniego poziomu dokÅ‚adnoÅ›ci.





### **WzÃ³r metody Gaussa-Seidla**  
Metoda Gaussa-Seidla to **iteracyjny sposÃ³b** rozwiÄ…zywania ukÅ‚adu rÃ³wnaÅ„ liniowych:

$$
A \mathbf{x} = \mathbf{b}
$$

gdzie:
- **$A$** â€“ macierz kwadratowa $n \times n$,
- **$\mathbf{x}$** â€“ wektor niewiadomych $x_1, x_2, ..., x_n$,
- **$\mathbf{b}$** â€“ wektor wyrazÃ³w wolnych.


Metoda Gaussa-Seidla rÃ³Å¼ni siÄ™ od metody Jacobiego tym, Å¼e podczas kaÅ¼dej iteracji **korzysta z najnowszych dostÄ™pnych wartoÅ›ci niewiadomych** â€“ zamiast opieraÄ‡ siÄ™ wyÅ‚Ä…cznie na wartoÅ›ciach z poprzedniej iteracji.

---

### **WzÃ³r iteracyjny metody Gaussa-Seidla:**
$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \right), \quad i = 1, 2, \dots, n
$$

---

 
### **WyjaÅ›nienie oznaczeÅ„:**
1. **$x_i^{(k+1)}$** â€“ nowe przybliÅ¼enie wartoÅ›ci niewiadomej $x_i$ w iteracji $k+1$.
2. **$a_{ii}$** â€“ element diagonalny macierzy $A$, czyli $i$-ta wartoÅ›Ä‡ na gÅ‚Ã³wnej przekÄ…tnej.
3. **$b_i$** â€“ $i$-ta wartoÅ›Ä‡ wektora $\mathbf{b}$ (wyrazy wolne).
4. **$a_{ij}$** â€“ element macierzy $A$ (dla $j \neq i$, czyli poza diagonalÄ…).
5. **$x_j^{(k+1)}$** â€“ wartoÅ›ci niewiadomych, ktÃ³re **juÅ¼ zostaÅ‚y obliczone w bieÅ¼Ä…cej iteracji** (dla $j < i$).
6. **$x_j^{(k)}$** â€“ wartoÅ›ci niewiadomych, ktÃ³re **nie zostaÅ‚y jeszcze zaktualizowane**, wiÄ™c wciÄ…Å¼ korzystamy z wartoÅ›ci z poprzedniej iteracji (dla $j > i$).
7. **$i$** â€“ indeks numeru rÃ³wnania i zmiennej, ktÃ³rÄ… obliczamy w danym kroku iteracyjnym
   - W ukÅ‚adzie rÃ³wnaÅ„ mamy $n$ rÃ³wnaÅ„ i $n$ zmiennych.
   - $i$ przyjmuje wartoÅ›ci od 1 do $n$, oznaczajÄ…c kolejne zmienne $x_1, x_2, ..., x_n$.
   - Dla kaÅ¼dego $i$ obliczamy nowÄ… wartoÅ›Ä‡ $x_i^{(k+1)}$, **korzystajÄ…c z najnowszych dostÄ™pnych wartoÅ›ci**.
8. **$n$** â€“ liczba rÃ³wnaÅ„ i zmiennych w ukÅ‚adzie
   - Macierz $A$ ma wymiar $n \times n$, co oznacza, Å¼e mamy dokÅ‚adnie $n$ rÃ³wnaÅ„ i $n$ niewiadomych.
   - Wektor $\mathbf{x}$ zawiera $n$ zmiennych, ktÃ³re sÄ… iteracyjnie obliczane.
   - Proces iteracyjny wykonuje siÄ™ dla wszystkich $i = 1, 2, ..., n$, aÅ¼ do osiÄ…gniÄ™cia odpowiedniego poziomu dokÅ‚adnoÅ›ci.


---


### **WzÃ³r metody SOR (Successive Over-Relaxation)**  
Metoda **SOR (Successive Over-Relaxation)** to ulepszona wersja metody **Gaussa-Seidla**, w ktÃ³rej wprowadza siÄ™ wspÃ³Å‚czynnik relaksacji **$\omega$**, aby przyspieszyÄ‡ zbieÅ¼noÅ›Ä‡.

UkÅ‚ad rÃ³wnaÅ„ liniowych:

$$
A \mathbf{x} = \mathbf{b}
$$

gdzie:
- **$A$** â€“ macierz kwadratowa **$n \times n$**,
- **$\mathbf{x}$** â€“ wektor niewiadomych **$x_1, x_2, ..., x_n$**,
- **$\mathbf{b}$** â€“ wektor wyrazÃ³w wolnych.

---

### **WzÃ³r iteracyjny metody SOR:**
$$
x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \right), \quad i = 1, 2, \dots, n
$$


---

### **WyjaÅ›nienie oznaczeÅ„:**
1. **$x_i^{(k+1)}$** â€“ nowe przybliÅ¼enie wartoÅ›ci niewiadomej **$x_i$** w iteracji **$k+1$**.
2. **$a_{ii}$** â€“ element diagonalny macierzy **$A$**, czyli **$i$**-ta wartoÅ›Ä‡ na gÅ‚Ã³wnej przekÄ…tnej.
3. **$b_i$** â€“ **$i$**-ta wartoÅ›Ä‡ wektora **$\mathbf{b}$** (wyrazy wolne).
4. **$a_{ij}$** â€“ element macierzy **$A$** (dla **$j \neq i$**, czyli poza diagonalÄ…).
5. **$x_j^{(k+1)}$** â€“ wartoÅ›ci niewiadomych, ktÃ³re **juÅ¼ zostaÅ‚y obliczone w bieÅ¼Ä…cej iteracji** (dla **$j < i$**).
6. **$x_j^{(k)}$** â€“ wartoÅ›ci niewiadomych, ktÃ³re **nie zostaÅ‚y jeszcze zaktualizowane**, wiÄ™c wciÄ…Å¼ korzystamy z wartoÅ›ci z poprzedniej iteracji (dla **$j > i$**).
7. **$\omega$** â€“ wspÃ³Å‚czynnik relaksacji**
   - **$\omega \in (0,2)$** to parametr, ktÃ³ry wpÅ‚ywa na szybkoÅ›Ä‡ zbieÅ¼noÅ›ci metody.
   - Dla **$\omega = 1$** metoda SOR staje siÄ™ metodÄ… Gaussa-Seidla.
   - Dla **$\omega > 1$** metoda moÅ¼e szybciej zbiegaÄ‡ do rozwiÄ…zania (**nadrelaksacja**).
   - Dla **$0 < \omega < 1$** metoda stabilizuje obliczenia, ale moÅ¼e wolniej zbiegaÄ‡ (**podrelaksacja**).
8. **$i$** â€“ indeks numeru rÃ³wnania i zmiennej, ktÃ³rÄ… obliczamy w danym kroku iteracyjnym**
   - W ukÅ‚adzie rÃ³wnaÅ„ mamy **$n$** rÃ³wnaÅ„ i **$n$** zmiennych.
   - **$i$** przyjmuje wartoÅ›ci od 1 do **$n$**, oznaczajÄ…c kolejne zmienne **$x_1, x_2, ..., x_n$**.
   - Dla kaÅ¼dego **$i$** obliczamy nowÄ… wartoÅ›Ä‡ **$x_i^{(k+1)}$**, **korzystajÄ…c z najnowszych dostÄ™pnych wartoÅ›ci** oraz wspÃ³Å‚czynnika **$\omega$**.
9. **$n$** â€“ liczba rÃ³wnaÅ„ i zmiennych w ukÅ‚adzie**
   - Macierz **$A$** ma wymiar **$n \times n$**, co oznacza, Å¼e mamy dokÅ‚adnie **$n$** rÃ³wnaÅ„ i **$n$** niewiadomych.
   - Wektor **$\mathbf{x}$** zawiera **$n$** zmiennych, ktÃ³re sÄ… iteracyjnie obliczane.
   - Proces iteracyjny wykonuje siÄ™ dla wszystkich **$i = 1, 2, ..., n$**, aÅ¼ do osiÄ…gniÄ™cia odpowiedniego poziomu dokÅ‚adnoÅ›ci.

---


### ğŸ”¹ **Wyprowadzenie wzorÃ³w dla metod iteracyjnych Jacobiego i Gaussa-Seidla**

Metody iteracyjne, takie jak **Jacobi** i **Gauss-Seidel**, sÅ‚uÅ¼Ä… do rozwiÄ…zywania ukÅ‚adÃ³w rÃ³wnaÅ„ liniowych postaci:

$$
Ax = b
$$

gdzie:
- **$A$** jest macierzÄ… wspÃ³Å‚czynnikÃ³w (**$n \times n$**),
- **$x$** jest wektorem niewiadomych (**$n \times 1$**),
- **$b$** jest wektorem wynikÃ³w (**$n \times 1$**).

Metody te wyprowadzamy poprzez przeksztaÅ‚cenie ukÅ‚adu **$Ax = b$** w formÄ™ iteracyjnÄ….

---

## **1ï¸âƒ£ PodziaÅ‚ macierzy **$A$****
PodstawÄ… wyprowadzenia metod iteracyjnych jest **rozbicie macierzy **$A$**** na trzy czÄ™Å›ci:

$$
A = D + L + U
$$

gdzie:
- **$D$** to **macierz diagonalna** (zawierajÄ…ca tylko elementy na gÅ‚Ã³wnej przekÄ…tnej macierzy **$A$**),
- **$L$** to **macierz dolnotrÃ³jkÄ…tna** (zawierajÄ…ca tylko elementy poniÅ¼ej diagonali),
- **$U$** to **macierz gÃ³rnotrÃ³jkÄ…tna** (zawierajÄ…ca tylko elementy powyÅ¼ej diagonali).

PrzykÅ‚adowo, dla macierzy:

$$
A =
\begin{bmatrix}
4 & -1 & 0 & 0 \\
-1 & 4 & -1 & 0 \\
0 & -1 & 4 & -1 \\
0 & 0 & -1 & 3
\end{bmatrix}
$$

otrzymujemy:

$$
D =
\begin{bmatrix}
4 & 0 & 0 & 0 \\
0 & 4 & 0 & 0 \\
0 & 0 & 4 & 0 \\
0 & 0 & 0 & 3
\end{bmatrix}
$$

$$
L =
\begin{bmatrix}
0 & 0 & 0 & 0 \\
-1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0
\end{bmatrix}
$$

$$
U =
\begin{bmatrix}
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1 \\
0 & 0 & 0 & 0
\end{bmatrix}
$$

---

## **2ï¸âƒ£ Wyprowadzenie metody Jacobiego**
Wychodzimy od podstawowego rÃ³wnania macierzowego:

$$
Ax = b
$$

PodstawiajÄ…c **$A = D + L + U$**:

$$
(D + L + U)x = b
$$

Po przeksztaÅ‚ceniu:

$$
Dx = b - (L + U)x
$$

Teraz mnoÅ¼ymy obustronnie przez **$D^{-1}$**, co daje wzÃ³r iteracyjny:

$$
x^{(k+1)} = D^{-1} (b - (L + U)x^{(k)})
$$

Co w zapisie skÅ‚adowych wyglÄ…da nastÄ™pujÄ…co:

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right)
$$

### ğŸ”¹ **Interpretacja**
- KaÅ¼dy element **$x_i^{(k+1)}$** w nowym kroku zaleÅ¼y wyÅ‚Ä…cznie od **starych wartoÅ›ci **$x^{(k)}$****.
- Metoda Jacobiego traktuje kaÅ¼dÄ… rÃ³wnanie osobno, wykorzystujÄ…c wartoÅ›ci sprzed aktualizacji.
- **Jest Å‚atwa do rÃ³wnolegÅ‚ego przetwarzania**, poniewaÅ¼ nie wymaga natychmiastowego uÅ¼ycia nowych wartoÅ›ci.

---

## **3ï¸âƒ£ Wyprowadzenie metody Gaussa-Seidla**
Analogicznie do Jacobiego, zaczynamy od:

$$
Dx = b - (L + U)x
$$

Teraz jednak zamiast uÅ¼ywaÄ‡ tylko starych wartoÅ›ci **$x^{(k)}$**, moÅ¼emy **rozwiÄ…zywaÄ‡ ukÅ‚ad w miejscu** â€“ oznacza to, Å¼e uÅ¼ywamy juÅ¼ zaktualizowanych wartoÅ›ci, gdy tylko sÄ… dostÄ™pne. PrzeksztaÅ‚cajÄ…c:

$$
D x^{(k+1)} = b - L x^{(k+1)} - U x^{(k)}
$$

Po rozwiÄ…zaniu wzglÄ™dem **$x_i$**:

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j < i} a_{ij} x_j^{(k+1)} - \sum_{j > i} a_{ij} x_j^{(k)} \right)
$$

### ğŸ”¹ **Interpretacja**
- W porÃ³wnaniu do Jacobiego, wartoÅ›ci **natychmiast sÄ… aktualizowane** i uÅ¼ywane w nastÄ™pnych obliczeniach w tej samej iteracji.
- **Metoda Gaussa-Seidla zazwyczaj zbiega szybciej niÅ¼ Jacobi**, poniewaÅ¼ korzysta z najnowszych wartoÅ›ci w trakcie iteracji.

---

## **4ï¸âƒ£ PorÃ³wnanie metod**
| Cecha               | Metoda Jacobiego | Metoda Gaussa-Seidla |
|---------------------|----------------|----------------------|
| Wykorzystanie wartoÅ›ci | Stare wartoÅ›ci **$x^{(k)}$** | Natychmiast aktualizowane **$x^{(k+1)}$** |
| ZbieÅ¼noÅ›Ä‡          | Wolniejsza      | Szybsza            |
| MoÅ¼liwoÅ›Ä‡ rÃ³wnolegÅ‚oÅ›ci | Åatwiejsza | Trudniejsza |
| Warunki zbieÅ¼noÅ›ci | Macierz diagonalnie dominujÄ…ca lub symetryczna dodatnio okreÅ›lona | Podobne do Jacobiego, ale czasem dziaÅ‚a w szerszym zakresie |
| Implementacja | Prostsza | Bardziej skomplikowana |

---

## **5ï¸âƒ£ Kiedy metoda Jacobiego lub Gaussa-Seidla dziaÅ‚a dobrze?**

---

## **Podsumowanie**
- **Metody iteracyjne Jacobiego i Gaussa-Seidla** bazujÄ… na rozbiciu macierzy **$A = D + L + U$**.
- **Jacobi**: kaÅ¼da wartoÅ›Ä‡ **$x_i$** jest obliczana niezaleÅ¼nie, a nowe wartoÅ›ci aktualizowane dopiero po caÅ‚ej iteracji.
- **Gauss-Seidel**: nowe wartoÅ›ci **$x_i$** sÄ… natychmiast aktualizowane i wykorzystywane w dalszych obliczeniach w tej samej iteracji.
- **Gauss-Seidel zbiega szybciej niÅ¼ Jacobi**, ale Jacobi jest bardziej **przystosowany do obliczeÅ„ rÃ³wnolegÅ‚ych**.

